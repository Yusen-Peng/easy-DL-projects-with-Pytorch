# -*- coding: utf-8 -*-
"""digitrecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15byY3OK1JLWHM6kPzY-IncQj0npM-IS5
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torchvision
# %pip install torch

import torchvision
from torchvision import datasets
from torchvision.transforms import ToTensor


train_data = datasets.MNIST(
  root= 'data',
  train= True,
  transform= ToTensor(),
  download= True
  )

test_data = datasets.MNIST(
  root= 'data',
  train= False,
  transform= ToTensor(),
  download= True
  )

#check the shape
train_data.data.shape

#check the targets (digits)
train_data.targets

#define data loaders
from torch.utils.data import DataLoader
loaders = {
    'train': DataLoader(train_data,
                        batch_size=100,
                        shuffle=True,
                        num_workers=1),
    'test': DataLoader(test_data,
                        batch_size=100,
                        shuffle=True,
                        num_workers=1),
}
loaders

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class ConNet(nn.Module):
  def __init__(self):
    super(ConNet, self).__init__()

    # define convolutional layers
    self.conv1 = nn.Conv2d(1, 10, kernel_size=3)  # only one input channel
    self.pool = nn.MaxPool2d(2, 2)                # max-pooling layer
    self.conv2 = nn.Conv2d(10, 20, kernel_size=3)
    self.conv3 = nn.Conv2d(20, 20, kernel_size=3)

    # define fully-connected layers
    self.fc1 = nn.Linear(20*3*3, 100)
    self.fc2 = nn.Linear(100, 10)

  def forward(self, x):
    # 1 (1 color channel), 28, 28 (image size)
                                    #1, 28, 28
    x = F.relu(self.conv1(x))       #10, 26, 26
    x = self.pool(x)                #10, 13, 13
    x = F.relu(self.conv2(x))       #20, 11, 11
    x = self.pool(x)                #20, 5, 5
    x = F.relu(self.conv3(x))       #20, 3, 3
    x = torch.flatten(x, 1)         #180 (flattening process)
    x = F.relu(self.fc1(x))         #100
    x = self.fc2(x)                 #10
    return x

#hyper-parameters
num_epochs = 10
batch_size = 32
learning_rate = 0.001

#loss function and optimizer
model = ConNet()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# the training loop
for epoch in range(num_epochs):
    #iterate through all in the training loader
    for i, (images, labels) in enumerate(loaders['train']):

        # Forward propagation
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Back propagation and optimize
        loss.backward()
        optimizer.step()
        optimizer.zero_grad() # clear the gradient

# test the model
with torch.no_grad(): #deactivate gradient calculation
  n_correct = 0
  n_samples = len(loaders['test'].dataset)

  for images, labels in loaders['test']:
    outputs = model(images)

    #torch.max(outputs, 1) returns two tensors.
    #the first tensor contains the value of the highest probabilities
    #the second tensor contains the corresponding indices of these highest probabilities,
    #which indicate the predicted classes.
    _, predicted = torch.max(outputs, 1)
    n_correct += (predicted == labels).sum().item()

#calculate the accuracy
acc = n_correct / n_samples
print(f'Accuracy of the network on the {n_samples} test images: {100*acc} %')